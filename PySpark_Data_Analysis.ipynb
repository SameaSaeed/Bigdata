{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOY7WLk3mB+V1Gy1i8GgCMW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Kc0eN4y5L4FZ"},"outputs":[],"source":["# Import necessary libraries\n","from pyspark.sql import SparkSession\n","import logging\n","\n","# Initialize Spark session\n","spark = SparkSession.builder.appName(\"VideoStreamingETL\").getOrCreate()\n","\n","# Reduce verbose logging\n","spark.sparkContext.setLogLevel(\"ERROR\")\n","\n","# Configure logging to suppress warnings\n","logging.getLogger(\"py4j\").setLevel(logging.ERROR)"]},{"cell_type":"code","source":["viewing_history_path = \"video_streaming_data/viewing_history.csv\"\n","users_path = \"video_streaming_data/users.json\"\n","\n","viewing_history_df = spark.read.option(\"header\", \"true\").csv(viewing_history_path)\n","users_df = spark.read.option(\"multiline\", \"true\").json(users_path)"],"metadata":{"id":"ZTYGNr366WU9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Write data to Parquet format, Limit rows during development to reduce memory pressure in this lab environment\n","sample_df = viewing_history.orderBy(\"timestamp_column\").limit(100000)\n","\n","# Write to disk\n","sample_df.write.mode(\"overwrite\").parquet(f\"{processed_path}/viewing_history.parquet\")\n","\n","print(\" Viewing history successfully written to Parquet.\")"],"metadata":{"id":"8227xeKI7z_Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Write the sample data with partitioning\n","viewing_history.write.partitionBy(\"device_type\").mode(\"overwrite\").parquet(f\"{processed_path}/viewing_history_partitioned\")\n","print(\" viewing history data has been successfully written with partitioning!\")\n","\n","# Verify Partitioning by checking folder\n","import os\n","partitioned_path = f\"{processed_path}/viewing_history_partitioned\"\n","print(\"Partitions created:\", os.listdir(partitioned_path))\n","\n","# Verify partitioned column values\n","df_partitioned = spark.read.parquet(partitioned_path)\n","df_partitioned.select(\"device_type\").distinct().show()"],"metadata":{"id":"WVMadWoq75eQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Bucket data to sort by video_id making operations like joins or aggregations more efficient when filtering by video_id\n","\n","viewing_history.write.bucketBy(10, \"video_id\")\n",".sortBy(\"video_id\")\n",".mode(\"overwrite\")\n",".format(\"parquet\")\n",".option(\"path\", f\"{processed_path}/viewing_history_bucketed\")\n",".saveAsTable(\"viewing_history_bucketed\")\n","\n","print(\" Data successfully written with bucketing!\")\n","\n","# Check if the table exists\n","spark.catalog.listTables()\n","\n","# Verify bucketing in table schema\n","spark.sql(\"DESCRIBE FORMATTED viewing_history_bucketed\").show()\n","\n","# Read the table from Spark's Metastore\n","bucketed_df = spark.read.table(\"viewing_history_bucketed\")\n","bucketed_df.show(5)\n","\n","# Query execution time before bucketing\n","import time\n","from pyspark.sql.functions import col\n","\n","start_time = time.time()\n","viewing_history.filter(col(\"video_id\") == '56789').show()\n","end_time = time.time()\n","\n","print(f\" Query Execution Time (Before Bucketing): {end_time - start_time:.4f} seconds\")\n","\n","# Query execution time after bucketing\n","start_time = time.time()\n","spark.sql(\"SELECT * FROM viewing_history_bucketed WHERE video_id = '56789'\").show()\n","end_time = time.time()\n","\n","print(f\" Query Execution Time (After Bucketing): {end_time - start_time:.4f} seconds\")"],"metadata":{"id":"6bMGJDGk8arh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql.functions import col, sum as _sum, when\n","\n","# Get the Null counts in viewing_history\n","print(\" Null counts in viewing_history_df:\")\n","viewing_history_df.select([\n","_sum(when(col(c).isNull(), 1).otherwise(0)).alias(c)\n","for c in viewing_history_df.columns\n","]).show()\n","\n","# Get the Null counts in users_df\n","print(\" Null counts in users_df:\")\n","users_df.select([\n","_sum(when(col(c).isNull(), 1).otherwise(0)).alias(c)\n","for c in users_df.columns\n","]).show()\n","\n","# Fill missing device_type with \"Unknown\"\n","viewing_history_df = viewing_history_df.fillna({\"device_type\": \"Unknown\"})\n","\n","# Drop account_status column (mostly null)\n","if \"account_status\" in viewing_history_df.columns:\n","viewing_history_df = viewing_history_df.drop(\"account_status\")\n","\n","# Drop rows missing user_id or watched_at (critical fields)\n","viewing_history_df = viewing_history_df.dropna(subset=[\"user_id\", \"watched_at\"])\n","\n","# Fill preferred_language with \"Unknown\"\n","users_df = users_df.fillna({\"preferred_language\": \"Unknown\"})\n","\n","# Fill missing subscription_date with a placeholder\n","users_df = users_df.fillna({\"subscription_date\": \"2020-01-01\"})\n","# Verify by rerunning\n","from pyspark.sql.functions import col, sum as _sum, when"],"metadata":{"id":"7dnuCVpu6mi6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check for duplicates\n","\n","users_total = users_df.count()\n","unique_user_ids = users_df.select(\"user_id\").distinct().count()\n","\n","# Print total rows, distinct rows, and duplicate rows\n","print(f\"Total user records: {users_total}\")\n","print(f\"Unique user_id count: {unique_user_ids}\")\n","print(f\"Duplicate user rows: {users_total - unique_user_ids}\")\n","\n","# Remove duplicates based on user_id\n","users_df = users_df.dropDuplicates([\"user_id\"])\n","\n","# Confirm cleanup\n","print(f\" Users after deduplication: {users_df.count()} rows\")"],"metadata":{"id":"q9iEHiM66r8-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check the current schema\n","viewing_history_df.printSchema()\n","\n","from pyspark.sql.functions import to_timestamp, col\n","\n","# Convert user_id to IntegerType\n","viewing_history_df = viewing_history_df.withColumn(\"user_id\", col(\"user_id\").cast(\"int\"))\n","\n","# Convert watched_at to TimestampType\n","viewing_history_df = viewing_history_df.withColumn(\"watched_at\", to_timestamp(col(\"watched_at\")))\n","\n","# Confirm the changes\n","viewing_history_df.printSchema()\n","viewing_history_df.select(\"user_id\", \"watched_at\").show(5)"],"metadata":{"id":"jSn3NWLR6vcZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Optional: Cast user_id in both dataframes\n","viewing_history_df = viewing_history_df.withColumn(\"user_id\", col(\"user_id\").cast(\"int\"))\n","users_df = users_df.withColumn(\"user_id\", col(\"user_id\").cast(\"int\"))\n","\n","# Make sure both DataFrames are clean and have user_id\n","viewing_history_df.select(\"user_id\").show(3)\n","users_df.select(\"user_id\").show(3)\n","\n","# Join both DataFrames\n","full_df = viewing_history_df.join(users_df, on=\"user_id\", how=\"inner\")\n","\n","# Preview Results\n","full_df.select(\"user_id\", \"video_id\", \"watched_at\", \"email\", \"preferred_language\").show(5)"],"metadata":{"id":"eurkaYyI7KQT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Count Views per User per Video\n","from pyspark.sql.functions import count\n","\n","# Count how many times each user watched each video\n","most_watched_df = full_df.groupBy(\"user_id\", \"video_id\").agg(\n","count(\"*\").alias(\"watch_count\")\n",")\n","\n","# Show most watched videos overall\n","most_watched_df.orderBy(\"watch_count\", ascending=False).show(10)\n","\n","## Add Total Views per User\n","# Count total watch events per user\n","total_views_df = full_df.groupBy(\"user_id\").agg(\n","count(\"*\").alias(\"total_views\")\n",")\n","\n","# Show most active users\n","total_views_df.orderBy(\"total_views\", ascending=False).show(10)\n","\n","### Rank Top 3 Most-Watched Videos per User\n","from pyspark.sql.window import Window\n","from pyspark.sql.functions import row_number\n","\n","# Create window spec to rank videos within each user\n","window_spec = Window.partitionBy(\"user_id\").orderBy(col(\"watch_count\").desc())\n","\n","# Add ranking column\n","ranked_df = most_watched_df.withColumn(\"rank\", row_number().over(window_spec))\n","\n","# Filter to get top 3 videos per user\n","top_videos_per_user = ranked_df.filter(col(\"rank\") <= 3)\n","\n","# Show results\n","top_videos_per_user.orderBy(\"user_id\", \"rank\").show(10)"],"metadata":{"id":"IsiW6N9P7QEK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Preview device types\n","viewing_history_df.select(\"device_type\").distinct().show()\n","### Define a Python UDF to clean device labels\n","from pyspark.sql.functions import udf\n","from pyspark.sql.types import StringType\n","\n","# Define the normalization logic\n","def normalize_device(device):\n","if device is None:\n","return \"Unknown\"\n","device = device.lower()\n","if \"iphone\" in device:\n","return \"iPhone\"\n","elif \"android\" in device:\n","return \"Android\"\n","else:\n","return \"Other\"\n","\n","# Register as a PySpark UDF\n","normalize_device_udf = udf(normalize_device, StringType())\n","### Apply the UDF to your DataFrame\n","# Create a new column with normalized values\n","viewing_history_df = viewing_history_df.withColumn(\n","\"normalized_device\", normalize_device_udf(viewing_history_df[\"device_type\"])\n",")\n","### Inspect the result\n","viewing_history_df.select(\"device_type\", \"normalized_device\").distinct().show()"],"metadata":{"id":"yofmYlZB7bR0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" PySpark based on **RDDs**, provides **distributed** **computing** capabilities that **scale** **horizontally** across clusters, handling **terabytes** of data.\n","\n","1. Transformations and Actions:\n","Transformations (like select(), filter(), reshape()) are lazy operations that build execution plans\n","Actions (like show(), count()) trigger actual computation and return results\n","\n","2. Optimization Techniques:\n","Predicate pushdown, column pruning, and caching to improve performance\n","Partitioning strategies for efficient data storage and retrieval\n","Execution plan analysis for query optimization\n","\n","3. Advanced Analytics:\n","Window functions for complex analytical queries\n","Pivot operations for reshaping datasets\n","Approximation algorithms for efficient large-scale analytics"],"metadata":{"id":"LKro0BxMMSfV"}},{"cell_type":"markdown","source":["Session"],"metadata":{"id":"jy4Xbtu9z555"}},{"cell_type":"code","source":["#Session\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName(\"PySpark Transformations\").getOrCreate()"],"metadata":{"id":"vTNjxbMnN3nn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Loading Data"],"metadata":{"id":"4I46I1na_aH8"}},{"cell_type":"code","source":["from pyspark.storagelevel import StorageLevel\n","\n","# Parquet is a columnar storage format that offers efficient compression and encoding schemes, making it ideal for big data processing.\n","def load_dataframe(spark):\n","    df = spark.read.parquet(\"transaction_data.parquet\")\n","    return df, df.storageLevel\n","\n","df, storage_level = load_dataframe(spark)\n","print(\"Storage level before caching:\", storage_level)\n"],"metadata":{"id":"tOl9iS-nOvem"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Caching"],"metadata":{"id":"G2QH33yOzx1r"}},{"cell_type":"code","source":["# Default caching is equivalent to df.persist(StorageLevel.MEMORY_AND_DISK) Other Options are: MEMORY_ONLY, MEMORY_AND_DISK, DISK_ONLY\n","def cache_dataframe(df):\n","    df.cache()\n","    df.count()\n","    is_cached = df.storageLevel != StorageLevel.NONE\n","    return df, is_cached, df.storageLevel\n","\n","df, is_cached, storage_level = cache_dataframe(df)\n","print(\"Is DataFrame cached:\", is_cached)\n","print(\"Storage level after caching:\", storage_level)"],"metadata":{"id":"5Z42dH-3zzSu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Debugging"],"metadata":{"id":"OcBTp0fP2Elq"}},{"cell_type":"code","source":["# Understanding execution plans helps identify bottlenecks and helps in query Debugging\n","def exe_plan(df):\n","  plan_str = df._jdf.queryExecution().toString()\n","  return plan_str"],"metadata":{"id":"3KspsWUE1qzC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Transformation"],"metadata":{"id":"FFn1kd20_Qs_"}},{"cell_type":"code","source":[" # Column Pruning, Partitioning, Predicate pushdown(filter rows on condition)\n"," def filter_df(df):\n","  selected_df = df.select(\"transaction_id\", \"customer_id\", \"amount\", \"category\", \"transaction_date\")\n","  df_with_partitions = selected_df.withColumn(\"year\", year(df[\"transaction_date\"])).withColumn(\"month\", month(df[\"transaction_date\"]))\n","  df_with_partitions.write.partitionBy(\"year\", \"month\").mode(\"overwrite\").parquet(output_path)\n","  filtered_df = df_with_partitions.filter(col(\"amount\") > 50)\n","  filtered_df.explain(extended=True)\n","# The .explain() method shows the logical and physical plans for the query. Look for terms like \"PushedFilters\" or \"PartitionFilters\" in the plan to confirm pushdown.\n","  category_filtered_df = filtered_df.filter(col(\"category\").isin([\"Clothes\", \"Electronics\"]))\n","  return category_filtered_df\n","\n"," # Data Enrichment\n","def enrich_df(df):\n","  df_with_rounded = df.withColumn(\"rounded_amount\", round(col(\"amount\"), 0))\n","  df_with_desc = df_with_rounded.withColumn(\"transaction_desc\", concat(lit(\"Transaction #\"), col(\"transaction_id\"), lit(\" - \"), col(\"category\")))\n","  df_dropped = df_with_desc.drop(\"amount\")\n","  df_renamed = df_dropped.withColumnRenamed(\"rounded_amount\", \"amount_rounded\")\n","  return df_renamed"],"metadata":{"id":"mlKe9bRUPVob"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Analyzing"],"metadata":{"id":"NRsalCqm_HIp"}},{"cell_type":"code","source":["# Aggregation (Groupby Operation and Stats functions)\n","def aggregate_df(df):\n","  aggregated_df = df.groupBy(\"category\").agg(\n","          F.count(\"transaction_id\").alias(\"transactioner\"),\n","          F.sum(\"amount\").cast(DoubleType()).alias(\"total_amount\"),\n","          F.avg(\"amount\").cast(DoubleType()).alias(\"average_amount\"),\n","          F.max(\"amount\").cast(DoubleType()).alias(\"max_amount\"),\n","          F.min(\"amount\").cast(DoubleType()).alias(\"min_amount\")\n","  )\n","  sorted_df = aggregated_df.orderBy(F.desc(\"total_amount\")\n","  return sorted_df\n","\n","  df.groupBy('category').agg(countDistinct('transaction_id').alias('transactioner')).orderBy(desc('transactioner')).show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":106},"id":"0DcHtxkI2Xn3","executionInfo":{"status":"error","timestamp":1762363205136,"user_tz":-300,"elapsed":118,"user":{"displayName":"Samia Saeed","userId":"06339620815291903302"}},"outputId":"ba44e393-f427-4a16-d0c8-2a87dd0ceac1"},"execution_count":null,"outputs":[{"output_type":"error","ename":"IndentationError","evalue":"unexpected indent (ipython-input-3552336789.py, line 10)","traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-3552336789.py\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    sorted_df = aggregated_df.orderBy(F.desc(\"total_amount\")\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"]}]},{"cell_type":"code","source":["# Window functions for data partitioning (ranking, percentiles, and moving aggregations with row context and SQL OVER Clause)\n","window_spec = Window.partitionBy(\"category\").orderBy(desc(\"amount\"))\n","windowed_df = df.withColumn(\"rank\", rank().over(window_spec)).withColumn(\"dense_rank\", dense_rank().over(window_spec)).withColumn(\"row_number\", row_number().over(window_spec))\n","top_transactions = windowed_df.filter(col(\"rank\") <= 3)\n","return top_transactions"],"metadata":{"id":"AixwTVPpAEJg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Pivot Operation: Pivoting can be expensive for large datasets because it may cause significant data shuffling. Limiting the rows (with .limit(50000)) is one way to mitigate this for large datasets\n","def pivot_df(df):\n","  limited_df = df.limit(50000)\n","  pivoted_df = limited_df.groupBy(\"customer_id\").pivot(\"category\").agg(sum(\"amount\").cast(DoubleType()))\n","  categories = limited_df.select(\"category\").distinct().collect()\n","  category_names = [row[\"category\"] for row in categories]\n","  for category in category_names:\n","  pivoted_df = pivoted_df.fillna({category: 0.0})\n","  return pivoted_df"],"metadata":{"id":"8IyibR1yCLRI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Approximation algorithms\n","from pyspark.sql.functions import approx_count_distinct, percentile_approx, array, lit\n","def approx_df(df):\n","  approx_df = df.groupBy(\"category\").agg(\n","  approx_count_distinct(\"customer_id\").alias(\"approx_distinct_customers\"),percentile_approx(\"amount\", 0.5, 100).alias(\"median_amount\"),percentile_approx(\"amount\", array(lit(0.25), lit(0.5), lit(0.75)), 100).alias(\"quartiles\"))\n","  return approx_df"],"metadata":{"id":"OrJPjAnuDC9i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#  Summary statistics for Exploratory Analysis & data quality issues like outliers, missing values, or unexpected distributions.\n","def summarize_df(df):\n","  summary_df = df.select(\"amount\").summary(\"count\", \"mean\", \"stddev\", \"min\", \"25%\", \"50%\", \"75%\", \"max\")\n","  return summary_df"],"metadata":{"id":"XZXwEq-eDrPA"},"execution_count":null,"outputs":[]}]}